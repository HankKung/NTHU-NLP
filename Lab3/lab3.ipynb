{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import jieba\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-train\n",
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, '/Users/7ckung/Downloads/glove.6B')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup')\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 10  # Number of epochs to train for.\n",
    "latent_dim = 100  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'cmn-eng/cmn.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/mf/4ngqf9b134d2m4zww7nmx6fc0000gn/T/jieba.cache\n",
      "Loading model cost 0.777 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_words = set()\n",
    "target_words = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    #target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for word in word_tokenize(input_text):\n",
    "        if word not in input_words:\n",
    "            input_words.add(word)\n",
    "    for word in list(jieba.cut(target_text)):\n",
    "        if word not in target_words:\n",
    "            target_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lake',\n",
       " 'Would',\n",
       " 'backed',\n",
       " 'drank',\n",
       " 'sponge',\n",
       " 'saw',\n",
       " 'crazy',\n",
       " 'skate',\n",
       " 'Chinese',\n",
       " 'knee',\n",
       " 'absorbed',\n",
       " 'talk',\n",
       " 'Jealousy',\n",
       " 'yard',\n",
       " 'dropped',\n",
       " 'radio',\n",
       " 'fellow',\n",
       " 'Divide',\n",
       " 'throne',\n",
       " 'trust',\n",
       " 'wanting',\n",
       " 'species',\n",
       " 'sea',\n",
       " 'Stop',\n",
       " 'synchronize',\n",
       " 'history',\n",
       " 'forty',\n",
       " 'An',\n",
       " 'imitation',\n",
       " 'Mexico',\n",
       " 'marry',\n",
       " 'cattle',\n",
       " 'brother',\n",
       " '90',\n",
       " 'team',\n",
       " 'artificial',\n",
       " 'drives',\n",
       " 'shop',\n",
       " 'Add',\n",
       " 'mend',\n",
       " 'undressing',\n",
       " 'test',\n",
       " 'elated',\n",
       " 'hoped',\n",
       " 'blamed',\n",
       " 'exploded',\n",
       " 'abandoned',\n",
       " 'kick',\n",
       " 'resembles',\n",
       " 'dining',\n",
       " 'stomachache',\n",
       " 'password',\n",
       " 'antiques',\n",
       " 'discount',\n",
       " 'afraid',\n",
       " 'thirty-one',\n",
       " 'guaranteed',\n",
       " 'clever',\n",
       " 'kitten',\n",
       " 'tallest',\n",
       " 'thieves',\n",
       " 'putt',\n",
       " 'planning',\n",
       " 'third',\n",
       " '6',\n",
       " 'watering',\n",
       " 'Spanish',\n",
       " 'directly',\n",
       " 'pizza',\n",
       " 'lights',\n",
       " 'room',\n",
       " 'disaster',\n",
       " 'progress',\n",
       " 'Day',\n",
       " 'hurts',\n",
       " 'depend',\n",
       " 'homesick',\n",
       " 'Canadians',\n",
       " 'Electricity',\n",
       " 'York',\n",
       " 'dance',\n",
       " 'flows',\n",
       " 'clubs',\n",
       " 'steps',\n",
       " 'breaking',\n",
       " 'useless',\n",
       " 'elderly',\n",
       " 'smart',\n",
       " 'repay',\n",
       " 'rates',\n",
       " 'rain',\n",
       " 'seemed',\n",
       " 'saluted',\n",
       " 'high',\n",
       " 'even',\n",
       " 'teams',\n",
       " 'headache',\n",
       " 'pretended',\n",
       " 'excuses',\n",
       " 'obey',\n",
       " 'procrastinating',\n",
       " 'cheated',\n",
       " 'potatoes',\n",
       " 'salesman',\n",
       " 'hiccups',\n",
       " 'criticizing',\n",
       " 'half',\n",
       " 'flowers',\n",
       " 'occasionally',\n",
       " 'new',\n",
       " 'gray',\n",
       " 'location',\n",
       " 'rung',\n",
       " 'carpet',\n",
       " 'point-blank',\n",
       " 'audience',\n",
       " 'earthquakes',\n",
       " 'Wait',\n",
       " 'came',\n",
       " 'unusual',\n",
       " 'States',\n",
       " 'feels',\n",
       " 'cute',\n",
       " 'lawn',\n",
       " 'tire',\n",
       " 'Stuff',\n",
       " 'tells',\n",
       " 'carefully',\n",
       " 'gardening',\n",
       " 'perfect',\n",
       " 'Wake',\n",
       " 'pregnant',\n",
       " 'dolls',\n",
       " 'exhibition',\n",
       " 'expert',\n",
       " 'kissing',\n",
       " 'worst',\n",
       " 'classmate',\n",
       " 'clothes',\n",
       " 'Denmark',\n",
       " 'September',\n",
       " 'release',\n",
       " 'Dead',\n",
       " 'soil',\n",
       " '839',\n",
       " 'looking',\n",
       " 'annoyed',\n",
       " 'lunch',\n",
       " 'guns',\n",
       " 'herself',\n",
       " 'dial',\n",
       " 'cholesterol',\n",
       " 'toward',\n",
       " 'trouble',\n",
       " 'Vietnam',\n",
       " 'silence',\n",
       " 'leaves',\n",
       " 'quotes',\n",
       " 'envelope',\n",
       " 'money',\n",
       " 'favorite',\n",
       " 'none',\n",
       " 'Try',\n",
       " 'gambling',\n",
       " 'Call',\n",
       " 'telephone',\n",
       " 'normal',\n",
       " 'ringing',\n",
       " 'Iron',\n",
       " 'extra',\n",
       " 'alike',\n",
       " 'roof',\n",
       " 'sloppy',\n",
       " 'board',\n",
       " 'joking',\n",
       " 'being',\n",
       " 'tape',\n",
       " 'his',\n",
       " 'works',\n",
       " 'profession',\n",
       " 'earns',\n",
       " 'Once',\n",
       " 'Jack',\n",
       " 'friendly',\n",
       " 'years',\n",
       " 'greedy',\n",
       " 'party',\n",
       " 'coach',\n",
       " 'coincidence',\n",
       " 'maybe',\n",
       " 'particularly',\n",
       " 'down',\n",
       " 'Canadian',\n",
       " 'either',\n",
       " 'weed',\n",
       " 'lately',\n",
       " 'meddling',\n",
       " 'positive',\n",
       " 'opened',\n",
       " 'notes',\n",
       " 'spent',\n",
       " 'wounded',\n",
       " 'gifted',\n",
       " 'drive',\n",
       " '65',\n",
       " 'Cheers',\n",
       " 'bug',\n",
       " 'note',\n",
       " 'watch',\n",
       " 'wealth',\n",
       " 'impatient',\n",
       " 'containers',\n",
       " 'Did',\n",
       " 'pushed',\n",
       " 'orange',\n",
       " 'quiet',\n",
       " 'Dog',\n",
       " 'expired',\n",
       " 'status',\n",
       " 'loudly',\n",
       " 'driving',\n",
       " 'scare',\n",
       " 'disappoint',\n",
       " 'Winds',\n",
       " 'watched',\n",
       " 'watches',\n",
       " 'amazingly',\n",
       " 'natural',\n",
       " 'None',\n",
       " 'pulled',\n",
       " '?',\n",
       " 'so',\n",
       " 'Whose',\n",
       " 'willing',\n",
       " 'bell',\n",
       " 'stars',\n",
       " 'Spring',\n",
       " 'towel',\n",
       " 'Japanese',\n",
       " 'gone',\n",
       " 'prohibited',\n",
       " 'bathe',\n",
       " 'shopping',\n",
       " 'Chiba',\n",
       " 'hear',\n",
       " 'having',\n",
       " 'sock',\n",
       " 'Others',\n",
       " 'temperature',\n",
       " 'responsible',\n",
       " 'subject',\n",
       " 'morning',\n",
       " 'escape',\n",
       " 'Thanks',\n",
       " 'beer',\n",
       " 'green',\n",
       " 'impressed',\n",
       " 'studying',\n",
       " 'Italy',\n",
       " 'easy',\n",
       " 'dealt',\n",
       " 'aches',\n",
       " 'umbrella',\n",
       " 'believe',\n",
       " 'intend',\n",
       " 'jumped',\n",
       " 'tickets',\n",
       " 'slavery',\n",
       " 'door',\n",
       " 'Park',\n",
       " 'armed',\n",
       " 'ruined',\n",
       " 'familiar',\n",
       " 'appreciate',\n",
       " 'meaningful',\n",
       " 'running',\n",
       " 'boyfriend',\n",
       " 'qualified',\n",
       " 'unfortunately',\n",
       " 'Please',\n",
       " 'p.m',\n",
       " 'saxophone',\n",
       " 'hero',\n",
       " 'depressing',\n",
       " 'slept',\n",
       " 'lot',\n",
       " 'worried',\n",
       " 'Both',\n",
       " 'thanked',\n",
       " 'resembled',\n",
       " 'chocolate',\n",
       " 'keep',\n",
       " 'Long',\n",
       " 'cart',\n",
       " 'vulnerable',\n",
       " 'monkey',\n",
       " \"'re\",\n",
       " 'soup',\n",
       " 'drugs',\n",
       " 'rained',\n",
       " 'bleeding',\n",
       " 'experience',\n",
       " 'when',\n",
       " 'noise',\n",
       " 'theater',\n",
       " 'responsibilities',\n",
       " 'Say',\n",
       " 'fooling',\n",
       " 'supposed',\n",
       " 'pains',\n",
       " 'rush',\n",
       " 'Detroit',\n",
       " 'Singapore',\n",
       " 'goof',\n",
       " 'analyzed',\n",
       " 'one',\n",
       " 'punished',\n",
       " 'painting',\n",
       " 'richer',\n",
       " 'ask',\n",
       " 'child',\n",
       " 'surprised',\n",
       " 'soldier',\n",
       " 'begin',\n",
       " 'languages',\n",
       " 'engineer',\n",
       " 'debate',\n",
       " 'Our',\n",
       " 'resisting',\n",
       " 'bankrupt',\n",
       " 'rude',\n",
       " 'World',\n",
       " 'handles',\n",
       " 'shalt',\n",
       " 'sent',\n",
       " 'Sheep',\n",
       " 'Europe',\n",
       " 'Food',\n",
       " 'sad',\n",
       " 'quite',\n",
       " 'coats',\n",
       " 'belongs',\n",
       " 'loses',\n",
       " 'elegant',\n",
       " 'kid',\n",
       " 'consider',\n",
       " 'know',\n",
       " 'panic',\n",
       " 'locked',\n",
       " 'suggested',\n",
       " 'injured',\n",
       " 'agreeable',\n",
       " 'millionaire',\n",
       " 'loves',\n",
       " 'winner',\n",
       " 'rising',\n",
       " 'polar',\n",
       " 'skirt',\n",
       " 'sell',\n",
       " 'video',\n",
       " 'pours',\n",
       " 'pain',\n",
       " 'managed',\n",
       " 'cooked',\n",
       " 'fear',\n",
       " 'Reading',\n",
       " 'immediately',\n",
       " 'public',\n",
       " 'cook',\n",
       " 'neither',\n",
       " 'past',\n",
       " 'Work',\n",
       " 'jeans',\n",
       " 'mailing',\n",
       " 'loser',\n",
       " 'fluently',\n",
       " 'Flying',\n",
       " 'trap',\n",
       " 'barber',\n",
       " 'pan',\n",
       " 'ASAP',\n",
       " 'pilot',\n",
       " 'tonight',\n",
       " 'is',\n",
       " \"'ve\",\n",
       " 'address',\n",
       " 'answered',\n",
       " 'Man',\n",
       " 'rap',\n",
       " 'ships',\n",
       " 'Mozart',\n",
       " 'overnight',\n",
       " 'pants',\n",
       " 'mirror',\n",
       " 'Paper',\n",
       " 'only',\n",
       " 'tie',\n",
       " 'Summers',\n",
       " 'Anything',\n",
       " 'wonderful',\n",
       " 'arisen',\n",
       " 'occupied',\n",
       " 'heat',\n",
       " 'Indian',\n",
       " 'rose',\n",
       " 'original',\n",
       " 'OK',\n",
       " 'Men',\n",
       " 'overreacting',\n",
       " 'napkin',\n",
       " 'hometown',\n",
       " 'opposed',\n",
       " 'fan',\n",
       " 'mind',\n",
       " 'weak',\n",
       " 'discuss',\n",
       " 'envy',\n",
       " 'tooth',\n",
       " 'Kiss',\n",
       " 'letter',\n",
       " 'adopted',\n",
       " 'day',\n",
       " 'stuck',\n",
       " 'consoling',\n",
       " 'hamburgers',\n",
       " 'spy',\n",
       " 'polish',\n",
       " 'bugs',\n",
       " 'vase',\n",
       " 'ignored',\n",
       " 'Breathe',\n",
       " 'dirty',\n",
       " 'education',\n",
       " 'Breakfast',\n",
       " 'ripped',\n",
       " 'Chicago',\n",
       " 'sings',\n",
       " 'bedrooms',\n",
       " 'journalist',\n",
       " 'A',\n",
       " 'check',\n",
       " 'Mt',\n",
       " 'Fresh',\n",
       " 'fault',\n",
       " 'keeps',\n",
       " 'waiting',\n",
       " 'Thai',\n",
       " 'saying',\n",
       " 'well',\n",
       " 'moon',\n",
       " 'fashion',\n",
       " 'carry',\n",
       " 'teach',\n",
       " 'pirates',\n",
       " 'Underage',\n",
       " 'examined',\n",
       " 'overslept',\n",
       " 'illness',\n",
       " 'interesting',\n",
       " 'wait',\n",
       " 'interested',\n",
       " 'Interest',\n",
       " 'stylish',\n",
       " 'Green',\n",
       " 'Chopin',\n",
       " 'job',\n",
       " 'results',\n",
       " 'air',\n",
       " 'breast-feeding',\n",
       " 'hating',\n",
       " 'else',\n",
       " 'especially',\n",
       " 'glared',\n",
       " 'speeches',\n",
       " 'twelve',\n",
       " 'going',\n",
       " 'kids',\n",
       " 'dripping',\n",
       " 'proved',\n",
       " 'branch',\n",
       " 'than',\n",
       " 'choices',\n",
       " 'truly',\n",
       " 'abducted',\n",
       " 'or',\n",
       " 'odd',\n",
       " 'Madrid',\n",
       " 'runs',\n",
       " 'reply',\n",
       " 'hotels',\n",
       " 'distrusted',\n",
       " 'worse',\n",
       " 'ones',\n",
       " 'write',\n",
       " 'major',\n",
       " 'dangerous',\n",
       " 'dog',\n",
       " 'once',\n",
       " 'disagree',\n",
       " 'witch',\n",
       " 'Kyoto',\n",
       " 'exploding',\n",
       " 'miracles',\n",
       " 'slave',\n",
       " 'killed',\n",
       " 'racket',\n",
       " 'loss',\n",
       " 'accident',\n",
       " 'vain',\n",
       " 'jerk',\n",
       " 'people',\n",
       " 'rescue',\n",
       " 'blocked',\n",
       " 'kitchen',\n",
       " 'lend',\n",
       " 'upset',\n",
       " 'scolding',\n",
       " 'artists',\n",
       " 'at',\n",
       " 'sat',\n",
       " 'museum',\n",
       " 'Now',\n",
       " 'Balls',\n",
       " 'any',\n",
       " 'windy',\n",
       " 'newspapers',\n",
       " 'pencil',\n",
       " 'rabbi',\n",
       " 'college',\n",
       " 'chairperson',\n",
       " 'My',\n",
       " 'gawking',\n",
       " \"n't\",\n",
       " 'dove',\n",
       " 'correct',\n",
       " 'exciting',\n",
       " '20',\n",
       " 'language',\n",
       " 'did',\n",
       " 'safe',\n",
       " 'knocked',\n",
       " 'robbery',\n",
       " 'pencils',\n",
       " 'reserve',\n",
       " 'Fight',\n",
       " 'Treat',\n",
       " 'number',\n",
       " 'photo',\n",
       " 'doubling',\n",
       " 'neighbor',\n",
       " 'honest',\n",
       " 'pocket',\n",
       " 'gifts',\n",
       " 'extraordinary',\n",
       " 'picnic',\n",
       " 'downtown',\n",
       " 'confident',\n",
       " 'forever',\n",
       " 'granted',\n",
       " 'airplane',\n",
       " 'uniform',\n",
       " 'lift',\n",
       " 'Oh',\n",
       " 'concern',\n",
       " 'climbed',\n",
       " 'slowly',\n",
       " 'asking',\n",
       " 'gotten',\n",
       " 'rent',\n",
       " 'definitely',\n",
       " 'Pass',\n",
       " 'larger',\n",
       " 'Well',\n",
       " 'caught',\n",
       " 'chicken',\n",
       " 'cuter',\n",
       " 'lazy',\n",
       " 'concept',\n",
       " 'hardly',\n",
       " 'pity',\n",
       " 'tenor',\n",
       " 'won',\n",
       " 'hurt',\n",
       " 'Boys',\n",
       " 'belt',\n",
       " 'taller',\n",
       " 'boys',\n",
       " 'easily',\n",
       " 'helping',\n",
       " 'side',\n",
       " 'hug',\n",
       " 'flour',\n",
       " 'Remember',\n",
       " 'bears',\n",
       " 'hens',\n",
       " 'thousand',\n",
       " 'garden',\n",
       " 'us',\n",
       " 'obligation',\n",
       " 'Popcorn',\n",
       " 'bear',\n",
       " 'straight',\n",
       " 'insist',\n",
       " 'Lend',\n",
       " 'blood',\n",
       " 'deleted',\n",
       " 'lies',\n",
       " 'months',\n",
       " 'smaller',\n",
       " 'stranger',\n",
       " 'glanced',\n",
       " 'hid',\n",
       " 'lighter',\n",
       " 'Soccer',\n",
       " 'repairing',\n",
       " 'bitten',\n",
       " 'fabulous',\n",
       " 'eighty',\n",
       " 'pirate',\n",
       " 'bow',\n",
       " 'ten-minute',\n",
       " 'baseball',\n",
       " 'chose',\n",
       " 'numerous',\n",
       " 'Definitely',\n",
       " 'shake',\n",
       " 'apologetic',\n",
       " 'tastes',\n",
       " 'article',\n",
       " 'build',\n",
       " 'Hello',\n",
       " 'forgot',\n",
       " 'shaved',\n",
       " 'mayor',\n",
       " 'import',\n",
       " 'ridiculous',\n",
       " 'handwriting',\n",
       " 'hill',\n",
       " 'anymore',\n",
       " 'downs',\n",
       " 'stamp',\n",
       " 'haunted',\n",
       " 'Unbelievable',\n",
       " 'bored',\n",
       " 'loan',\n",
       " 'Better',\n",
       " 'foreigner',\n",
       " 'Wednesday',\n",
       " 'human',\n",
       " 'ate',\n",
       " 'handle',\n",
       " 'goodbye',\n",
       " 'Two',\n",
       " 'limitless',\n",
       " 'ignoring',\n",
       " 'changed',\n",
       " 'chocolates',\n",
       " 'slippers',\n",
       " 'again',\n",
       " 'meal',\n",
       " 'rest',\n",
       " 'backward',\n",
       " 'fly',\n",
       " 'wish',\n",
       " 'Wolves',\n",
       " 'file',\n",
       " 'betray',\n",
       " 'rich',\n",
       " 'Open',\n",
       " 'staring',\n",
       " 'empty',\n",
       " 'important',\n",
       " 'minutes',\n",
       " 'have',\n",
       " 'gold',\n",
       " 'Earth',\n",
       " 'complaining',\n",
       " 'October',\n",
       " 'shorter',\n",
       " 'shipping',\n",
       " 'checked',\n",
       " 'ball',\n",
       " 'substituted',\n",
       " 'homework',\n",
       " 'traveling',\n",
       " 'More',\n",
       " 'silly',\n",
       " 'Boston',\n",
       " 'stab',\n",
       " 'telling',\n",
       " 'Check',\n",
       " 'newspaper',\n",
       " 'turns',\n",
       " 'east',\n",
       " 'Winter',\n",
       " 'parents',\n",
       " 'selling',\n",
       " 'sheets',\n",
       " 'disregarded',\n",
       " 'sharp',\n",
       " '6:00',\n",
       " 'Shanghai',\n",
       " 'cancel',\n",
       " 'emerging',\n",
       " 'Step',\n",
       " 'music',\n",
       " 'anyone',\n",
       " 'nice',\n",
       " 'world',\n",
       " 'fact',\n",
       " 'Valuable',\n",
       " 'peace-loving',\n",
       " 'makeup',\n",
       " 'cameras',\n",
       " 'grade',\n",
       " 'eating',\n",
       " 'advance',\n",
       " 'voice',\n",
       " 'river',\n",
       " 'convenient',\n",
       " 'same',\n",
       " 'listen',\n",
       " 'washed',\n",
       " 'wears',\n",
       " 'boy',\n",
       " 'each',\n",
       " 'owned',\n",
       " \"'m\",\n",
       " 'flawed',\n",
       " 'station',\n",
       " 'Time',\n",
       " 'walking',\n",
       " 'band',\n",
       " 'able',\n",
       " 'army',\n",
       " 'appears',\n",
       " 'danger',\n",
       " 'ten',\n",
       " 'anywhere',\n",
       " 'trash',\n",
       " 'figures',\n",
       " 'afford',\n",
       " 'ring',\n",
       " 'China',\n",
       " 'offer',\n",
       " 'before',\n",
       " 'teens',\n",
       " 'feeling',\n",
       " 'course',\n",
       " 'hat',\n",
       " 'guide',\n",
       " 'awfully',\n",
       " \"'d\",\n",
       " 'writes',\n",
       " 'woman',\n",
       " 'share',\n",
       " 'twin',\n",
       " 'growing',\n",
       " 'Who',\n",
       " 'lean',\n",
       " 'clean',\n",
       " 'how',\n",
       " 'acid',\n",
       " 'baked',\n",
       " 'desk',\n",
       " 'perfectly',\n",
       " 'plenty',\n",
       " 'Wood',\n",
       " 'motorcycle',\n",
       " 'dream',\n",
       " 'Rome',\n",
       " 'you',\n",
       " 'knew',\n",
       " 'floor',\n",
       " 'allowed',\n",
       " 'mad',\n",
       " 'arm',\n",
       " 'disposal',\n",
       " 'page',\n",
       " 'gun',\n",
       " 'gets',\n",
       " 'loved',\n",
       " 'juice',\n",
       " 'Sometimes',\n",
       " 'close',\n",
       " 'pink',\n",
       " 'musicians',\n",
       " 'Nara',\n",
       " 'circle',\n",
       " 'pet',\n",
       " 'escaping',\n",
       " 'Fill',\n",
       " 'fix',\n",
       " 'incalculable',\n",
       " 'Join',\n",
       " 'stopping',\n",
       " 'Harajuku',\n",
       " 'English',\n",
       " 'acres',\n",
       " 'dinner',\n",
       " 'word',\n",
       " 'rubbish',\n",
       " 'Sorry',\n",
       " 'talking',\n",
       " 'amazed',\n",
       " 'time',\n",
       " 'cut',\n",
       " 'suicide',\n",
       " 'delayed',\n",
       " 'Taiwan',\n",
       " 'gay',\n",
       " 'a',\n",
       " 'add',\n",
       " 'blast',\n",
       " 'soap',\n",
       " 'cycling',\n",
       " 'undo',\n",
       " 'serious',\n",
       " 'pick',\n",
       " 'clouds',\n",
       " 'fighting',\n",
       " 'Send',\n",
       " 'fine',\n",
       " 'Hungarian',\n",
       " 'amount',\n",
       " 'disturb',\n",
       " 'Has',\n",
       " 'His',\n",
       " 'Nice',\n",
       " 'almost',\n",
       " 'rang',\n",
       " 'drum',\n",
       " 'embraced',\n",
       " 'support',\n",
       " 'ill',\n",
       " 'aside',\n",
       " 'Mom',\n",
       " 'need',\n",
       " 'popcorn',\n",
       " 'students',\n",
       " 'lamp',\n",
       " 'melons',\n",
       " 'apologized',\n",
       " 'phone',\n",
       " 'excuse',\n",
       " 'admire',\n",
       " 'deaf',\n",
       " 'floors',\n",
       " 'pharmacy',\n",
       " 'lent',\n",
       " 'broken',\n",
       " 'postcard',\n",
       " 'needed',\n",
       " 'relationship',\n",
       " 'Asama',\n",
       " 'yen',\n",
       " 'love',\n",
       " 'counting',\n",
       " 'tends',\n",
       " 'tall',\n",
       " 'grades',\n",
       " 'after',\n",
       " 'dark',\n",
       " '9',\n",
       " 'rely',\n",
       " 'tolerated',\n",
       " 'cleared',\n",
       " 'liked',\n",
       " 'appeal',\n",
       " 'Wo',\n",
       " 'lasted',\n",
       " 'campus',\n",
       " 'happened',\n",
       " 'Animals',\n",
       " 'sang',\n",
       " 'try',\n",
       " 'myself',\n",
       " 'Boil',\n",
       " 'Read',\n",
       " 'change',\n",
       " 'daughters',\n",
       " 'star',\n",
       " '25th',\n",
       " 'decided',\n",
       " 'mouth',\n",
       " 'pens',\n",
       " 'nephew',\n",
       " 'medicine',\n",
       " 'good-looking',\n",
       " 'attacked',\n",
       " 'club',\n",
       " 'Italian',\n",
       " 'contain',\n",
       " 'Singing',\n",
       " 'forest',\n",
       " 'Stand',\n",
       " 'Control',\n",
       " 'rock',\n",
       " 'spell',\n",
       " 'Humor',\n",
       " 'pigsty',\n",
       " 'coward',\n",
       " '119',\n",
       " 'Go',\n",
       " 'Throw',\n",
       " 'treat',\n",
       " 'match',\n",
       " 'deserve',\n",
       " 'read',\n",
       " 'Shut',\n",
       " 'approved',\n",
       " 'itself',\n",
       " 'overcoat',\n",
       " 'paintings',\n",
       " 'belong',\n",
       " 'parasol',\n",
       " 'evening',\n",
       " 'miles',\n",
       " 'Knowledge',\n",
       " 'learn',\n",
       " 'scar',\n",
       " 'At',\n",
       " 'lay',\n",
       " 'dress',\n",
       " 'Englishman',\n",
       " 'meth',\n",
       " '%',\n",
       " 'dollars',\n",
       " 'bring',\n",
       " 'Restart',\n",
       " 'warmed',\n",
       " 'skiing',\n",
       " 'frightened',\n",
       " 'about',\n",
       " 'costs',\n",
       " 'relieved',\n",
       " 'cheek',\n",
       " 'fell',\n",
       " 'interest',\n",
       " 'piece',\n",
       " 'times',\n",
       " 'practical',\n",
       " 'market',\n",
       " 'confessed',\n",
       " 'wear',\n",
       " 'Most',\n",
       " 'fully',\n",
       " 'became',\n",
       " 'healthy',\n",
       " 'left-handed',\n",
       " 'bleed',\n",
       " 'brush',\n",
       " 'temper',\n",
       " 'stairs',\n",
       " 'Basketball',\n",
       " 'wo',\n",
       " 'stare',\n",
       " 'Selling',\n",
       " 'remained',\n",
       " 'Such',\n",
       " 'includes',\n",
       " 'hike',\n",
       " 'deceived',\n",
       " 'objected',\n",
       " ',',\n",
       " 'rather',\n",
       " 'math',\n",
       " 'counter',\n",
       " 'target',\n",
       " 'Keep',\n",
       " 'grapes',\n",
       " 'design',\n",
       " 'Arabic',\n",
       " 'fever',\n",
       " 'folding',\n",
       " '7',\n",
       " 'Smoking',\n",
       " 'honey',\n",
       " 'resting',\n",
       " 'expensive',\n",
       " '105',\n",
       " 'tags',\n",
       " 'Follow',\n",
       " 'inside',\n",
       " 'church',\n",
       " 'England',\n",
       " 'Your',\n",
       " 'bygones',\n",
       " 'speak',\n",
       " 'Those',\n",
       " 'Anybody',\n",
       " 'Watch',\n",
       " 'Is',\n",
       " 'shade',\n",
       " 'embarrassed',\n",
       " 'Almost',\n",
       " 'somebody',\n",
       " 'cleaners',\n",
       " 'provide',\n",
       " 'parachute',\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_words = sorted(list(input_words))\n",
    "target_words = sorted(list(target_words))\n",
    "num_encoder_tokens = len(input_words)\n",
    "num_decoder_tokens = len(target_words)\n",
    "max_encoder_seq_length = max([len(word_tokenize(txt)) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(list(jieba.cut(txt))) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 3874\n",
      "Number of unique output tokens: 6993\n",
      "Max sequence length for inputs: 12\n",
      "Max sequence length for outputs: 13\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(word, i) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict(\n",
    "    [(word, i) for i, word in enumerate(target_words)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, word in enumerate(word_tokenize(input_text)):\n",
    "        encoder_input_data[i, t, input_token_index[word]] = 1.\n",
    "    for t, word in enumerate(jieba.cut(target_text)):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[word]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[word]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " '$': 1,\n",
       " '%': 2,\n",
       " \"''\": 3,\n",
       " \"'d\": 4,\n",
       " \"'ll\": 5,\n",
       " \"'m\": 6,\n",
       " \"'re\": 7,\n",
       " \"'s\": 8,\n",
       " \"'ve\": 9,\n",
       " ',': 10,\n",
       " '.': 11,\n",
       " '1,000': 12,\n",
       " '10': 13,\n",
       " '10,000': 14,\n",
       " '100': 15,\n",
       " '105': 16,\n",
       " '10th': 17,\n",
       " '11': 18,\n",
       " '119': 19,\n",
       " '1636': 20,\n",
       " '17': 21,\n",
       " '18': 22,\n",
       " '1865': 23,\n",
       " '1945': 24,\n",
       " '1950': 25,\n",
       " '1970': 26,\n",
       " '1972': 27,\n",
       " '1988': 28,\n",
       " '1996': 29,\n",
       " '20': 30,\n",
       " '200': 31,\n",
       " '2003': 32,\n",
       " '2013': 33,\n",
       " '25th': 34,\n",
       " '2:30': 35,\n",
       " '3': 36,\n",
       " '3,000': 37,\n",
       " '30': 38,\n",
       " '3:30': 39,\n",
       " '4:00': 40,\n",
       " '5': 41,\n",
       " '58': 42,\n",
       " '6': 43,\n",
       " '65': 44,\n",
       " '6:00': 45,\n",
       " '6:30': 46,\n",
       " '7': 47,\n",
       " '7:00': 48,\n",
       " '7:30': 49,\n",
       " '8': 50,\n",
       " '800': 51,\n",
       " '839': 52,\n",
       " '8:30': 53,\n",
       " '9': 54,\n",
       " '90': 55,\n",
       " '9:00': 56,\n",
       " '?': 57,\n",
       " 'A': 58,\n",
       " 'ASAP': 59,\n",
       " 'Above': 60,\n",
       " 'Accidents': 61,\n",
       " 'Add': 62,\n",
       " 'Africa': 63,\n",
       " 'After': 64,\n",
       " 'All': 65,\n",
       " 'Allow': 66,\n",
       " 'Almost': 67,\n",
       " 'Always': 68,\n",
       " 'Am': 69,\n",
       " 'America': 70,\n",
       " 'American': 71,\n",
       " 'An': 72,\n",
       " 'Animals': 73,\n",
       " 'Anniversaries': 74,\n",
       " 'Answer': 75,\n",
       " 'Any': 76,\n",
       " 'Anybody': 77,\n",
       " 'Anyone': 78,\n",
       " 'Anything': 79,\n",
       " 'Anyway': 80,\n",
       " 'April': 81,\n",
       " 'Arabic': 82,\n",
       " 'Are': 83,\n",
       " 'Arithmetic': 84,\n",
       " 'As': 85,\n",
       " 'Asama': 86,\n",
       " 'Asia': 87,\n",
       " 'Ask': 88,\n",
       " 'Aso': 89,\n",
       " 'At': 90,\n",
       " 'August': 91,\n",
       " 'Australia': 92,\n",
       " 'Back': 93,\n",
       " 'Balls': 94,\n",
       " 'Banks': 95,\n",
       " 'Basho': 96,\n",
       " 'Basketball': 97,\n",
       " 'Be': 98,\n",
       " 'Bear': 99,\n",
       " 'Bears': 100,\n",
       " 'Beauty': 101,\n",
       " 'Behave': 102,\n",
       " 'Being': 103,\n",
       " 'Best': 104,\n",
       " 'Better': 105,\n",
       " 'Beware': 106,\n",
       " 'Bird': 107,\n",
       " 'Birds': 108,\n",
       " 'Black': 109,\n",
       " 'Boil': 110,\n",
       " 'Boston': 111,\n",
       " 'Both': 112,\n",
       " 'Boys': 113,\n",
       " 'Bread': 114,\n",
       " 'Break': 115,\n",
       " 'Breakfast': 116,\n",
       " 'Breathe': 117,\n",
       " 'Breathing': 118,\n",
       " 'Bring': 119,\n",
       " 'Britain': 120,\n",
       " 'But': 121,\n",
       " 'CD': 122,\n",
       " 'CDs': 123,\n",
       " 'Ca': 124,\n",
       " 'Call': 125,\n",
       " 'Can': 126,\n",
       " 'Canada': 127,\n",
       " 'Canadian': 128,\n",
       " 'Canadians': 129,\n",
       " 'Carrots': 130,\n",
       " 'Catch': 131,\n",
       " 'Cats': 132,\n",
       " 'Central': 133,\n",
       " 'Certainly': 134,\n",
       " 'Champagne': 135,\n",
       " 'Check': 136,\n",
       " 'Cheers': 137,\n",
       " 'Cheese': 138,\n",
       " 'Chiba': 139,\n",
       " 'Chicago': 140,\n",
       " 'Children': 141,\n",
       " 'China': 142,\n",
       " 'Chinatown': 143,\n",
       " 'Chinese': 144,\n",
       " 'Choose': 145,\n",
       " 'Chopin': 146,\n",
       " 'Christmas': 147,\n",
       " 'Classes': 148,\n",
       " 'Clean': 149,\n",
       " 'Clearly': 150,\n",
       " 'Close': 151,\n",
       " 'Clothes': 152,\n",
       " 'Coffee': 153,\n",
       " 'Come': 154,\n",
       " 'Congratulations': 155,\n",
       " 'Control': 156,\n",
       " 'Cooking': 157,\n",
       " 'Could': 158,\n",
       " 'Count': 159,\n",
       " 'Cows': 160,\n",
       " 'Cuff': 161,\n",
       " 'DJ': 162,\n",
       " 'Dad': 163,\n",
       " 'Dallas': 164,\n",
       " 'Day': 165,\n",
       " 'Days': 166,\n",
       " 'Dead': 167,\n",
       " 'Deal': 168,\n",
       " 'December': 169,\n",
       " 'Definitely': 170,\n",
       " 'Denmark': 171,\n",
       " 'Destroy': 172,\n",
       " 'Detroit': 173,\n",
       " 'Did': 174,\n",
       " 'Divide': 175,\n",
       " 'Do': 176,\n",
       " 'Does': 177,\n",
       " 'Dog': 178,\n",
       " 'Dogs': 179,\n",
       " 'Doing': 180,\n",
       " 'Dozens': 181,\n",
       " 'Draw': 182,\n",
       " 'Drink': 183,\n",
       " 'Drive': 184,\n",
       " 'Dry': 185,\n",
       " 'Dust': 186,\n",
       " 'Each': 187,\n",
       " 'Earth': 188,\n",
       " 'Eat': 189,\n",
       " 'Ebay': 190,\n",
       " 'Education': 191,\n",
       " 'Egypt': 192,\n",
       " 'Electricity': 193,\n",
       " 'England': 194,\n",
       " 'English': 195,\n",
       " 'Englishman': 196,\n",
       " 'Enjoy': 197,\n",
       " 'Europe': 198,\n",
       " 'Europeans': 199,\n",
       " 'Eve': 200,\n",
       " 'Even': 201,\n",
       " 'Every': 202,\n",
       " 'Everybody': 203,\n",
       " 'Everyone': 204,\n",
       " 'Everything': 205,\n",
       " 'Excuse': 206,\n",
       " 'Express': 207,\n",
       " 'Facebook': 208,\n",
       " 'Fancy': 209,\n",
       " 'Farsi': 210,\n",
       " 'Fasten': 211,\n",
       " 'Father': 212,\n",
       " 'Feel': 213,\n",
       " 'Felicja': 214,\n",
       " 'Few': 215,\n",
       " 'Fight': 216,\n",
       " 'Fill': 217,\n",
       " 'Finally': 218,\n",
       " 'Fire': 219,\n",
       " 'First': 220,\n",
       " 'Fish': 221,\n",
       " 'Flowers': 222,\n",
       " 'Fluency': 223,\n",
       " 'Flying': 224,\n",
       " 'Follow': 225,\n",
       " 'Food': 226,\n",
       " 'For': 227,\n",
       " 'Forewarned': 228,\n",
       " 'Foxes': 229,\n",
       " 'France': 230,\n",
       " 'Frankly': 231,\n",
       " 'French': 232,\n",
       " 'Fresh': 233,\n",
       " 'Friday': 234,\n",
       " 'From': 235,\n",
       " 'Fuji': 236,\n",
       " 'Fur': 237,\n",
       " 'German': 238,\n",
       " 'Germany': 239,\n",
       " 'Get': 240,\n",
       " 'Ghosts': 241,\n",
       " 'Give': 242,\n",
       " 'Go': 243,\n",
       " 'God': 244,\n",
       " 'Good': 245,\n",
       " 'Goodbye': 246,\n",
       " 'Goodbyes': 247,\n",
       " 'Google': 248,\n",
       " 'Grab': 249,\n",
       " 'Grandpa': 250,\n",
       " 'Great': 251,\n",
       " 'Greek': 252,\n",
       " 'Green': 253,\n",
       " 'Guess': 254,\n",
       " 'Hakone': 255,\n",
       " 'Halloween': 256,\n",
       " 'Hamlet': 257,\n",
       " 'Hands': 258,\n",
       " 'Hang': 259,\n",
       " 'Happy': 260,\n",
       " 'Harajuku': 261,\n",
       " 'Harvard': 262,\n",
       " 'Has': 263,\n",
       " 'Haste': 264,\n",
       " 'Have': 265,\n",
       " 'Hawaii': 266,\n",
       " 'He': 267,\n",
       " 'Health': 268,\n",
       " 'Hello': 269,\n",
       " 'Help': 270,\n",
       " 'Her': 271,\n",
       " 'Here': 272,\n",
       " 'Hey': 273,\n",
       " 'Hi': 274,\n",
       " 'Highway': 275,\n",
       " 'Hilton': 276,\n",
       " 'His': 277,\n",
       " 'Hokkaido': 278,\n",
       " 'Hold': 279,\n",
       " 'Honda': 280,\n",
       " 'Hop': 281,\n",
       " 'Hope': 282,\n",
       " 'Hotel': 283,\n",
       " 'How': 284,\n",
       " 'Hug': 285,\n",
       " 'Humor': 286,\n",
       " 'Hungarian': 287,\n",
       " 'Hunger': 288,\n",
       " 'Hurry': 289,\n",
       " 'I': 290,\n",
       " 'ID': 291,\n",
       " 'II': 292,\n",
       " 'Iceland': 293,\n",
       " 'If': 294,\n",
       " 'Ignorance': 295,\n",
       " 'Ignore': 296,\n",
       " 'In': 297,\n",
       " 'India': 298,\n",
       " 'Indian': 299,\n",
       " 'Industry': 300,\n",
       " 'Interest': 301,\n",
       " 'Iron': 302,\n",
       " 'Is': 303,\n",
       " 'It': 304,\n",
       " 'Italian': 305,\n",
       " 'Italy': 306,\n",
       " 'Itch': 307,\n",
       " 'Jack': 308,\n",
       " 'January': 309,\n",
       " 'Japan': 310,\n",
       " 'Japanese': 311,\n",
       " 'Jealousy': 312,\n",
       " 'Jesus': 313,\n",
       " 'John': 314,\n",
       " 'Join': 315,\n",
       " 'Just': 316,\n",
       " 'Keep': 317,\n",
       " 'Kiss': 318,\n",
       " 'Knowledge': 319,\n",
       " 'Kobe': 320,\n",
       " 'Korea': 321,\n",
       " 'Korean': 322,\n",
       " 'Kyoto': 323,\n",
       " 'L.A.': 324,\n",
       " 'Languages': 325,\n",
       " 'Large': 326,\n",
       " 'Latin': 327,\n",
       " 'Learning': 328,\n",
       " 'Leave': 329,\n",
       " 'Lemons': 330,\n",
       " 'Lend': 331,\n",
       " 'Lesson': 332,\n",
       " 'Let': 333,\n",
       " 'Liars': 334,\n",
       " 'Lie': 335,\n",
       " 'Life': 336,\n",
       " 'Lift': 337,\n",
       " 'Like': 338,\n",
       " 'Lincoln': 339,\n",
       " 'Listen': 340,\n",
       " 'Liverpool': 341,\n",
       " 'Liz': 342,\n",
       " 'Lock': 343,\n",
       " 'London': 344,\n",
       " 'Long': 345,\n",
       " 'Look': 346,\n",
       " 'Love': 347,\n",
       " 'Lunch': 348,\n",
       " 'MP3': 349,\n",
       " 'Madrid': 350,\n",
       " 'Mail': 351,\n",
       " 'Make': 352,\n",
       " 'Man': 353,\n",
       " 'Mandarin': 354,\n",
       " 'Manila': 355,\n",
       " 'Many': 356,\n",
       " 'Mario': 357,\n",
       " 'Mars': 358,\n",
       " 'Mary': 359,\n",
       " 'May': 360,\n",
       " 'Maybe': 361,\n",
       " 'Meet': 362,\n",
       " 'Men': 363,\n",
       " 'Mercury': 364,\n",
       " 'Merry': 365,\n",
       " 'Mexican': 366,\n",
       " 'Mexico': 367,\n",
       " 'Milton': 368,\n",
       " 'Mind': 369,\n",
       " 'Mine': 370,\n",
       " 'Mix': 371,\n",
       " 'Mom': 372,\n",
       " 'Mommy': 373,\n",
       " 'Monday': 374,\n",
       " 'Mondays': 375,\n",
       " 'Money': 376,\n",
       " 'More': 377,\n",
       " 'Most': 378,\n",
       " 'Mother': 379,\n",
       " 'Mount': 380,\n",
       " 'Move': 381,\n",
       " 'Mozart': 382,\n",
       " 'Mt': 383,\n",
       " 'My': 384,\n",
       " 'Nagasaki': 385,\n",
       " 'Nagoya': 386,\n",
       " 'Nara': 387,\n",
       " 'Nature': 388,\n",
       " 'Never': 389,\n",
       " 'New': 390,\n",
       " 'Next': 391,\n",
       " 'Nice': 392,\n",
       " 'No': 393,\n",
       " 'Nobody': 394,\n",
       " 'None': 395,\n",
       " 'Not': 396,\n",
       " 'Nothing': 397,\n",
       " 'Now': 398,\n",
       " 'OK': 399,\n",
       " 'Obey': 400,\n",
       " 'October': 401,\n",
       " 'Of': 402,\n",
       " 'Ogai': 403,\n",
       " 'Oh': 404,\n",
       " 'Old': 405,\n",
       " 'Once': 406,\n",
       " 'One': 407,\n",
       " 'Only': 408,\n",
       " 'Open': 409,\n",
       " 'Orange': 410,\n",
       " 'Ostriches': 411,\n",
       " 'Otaru': 412,\n",
       " 'Others': 413,\n",
       " 'Our': 414,\n",
       " 'Oxford': 415,\n",
       " 'PTA': 416,\n",
       " 'Pace': 417,\n",
       " 'Paper': 418,\n",
       " 'Parents': 419,\n",
       " 'Paris': 420,\n",
       " 'Park': 421,\n",
       " 'Parking': 422,\n",
       " 'Part': 423,\n",
       " 'Pass': 424,\n",
       " 'People': 425,\n",
       " 'Perfect': 426,\n",
       " 'Perhaps': 427,\n",
       " 'Pick': 428,\n",
       " 'Pizza': 429,\n",
       " 'Place': 430,\n",
       " 'Playing': 431,\n",
       " 'Please': 432,\n",
       " 'Pochi': 433,\n",
       " 'Police': 434,\n",
       " 'Popcorn': 435,\n",
       " 'Press': 436,\n",
       " 'Pretend': 437,\n",
       " 'Prices': 438,\n",
       " 'Pride': 439,\n",
       " 'Promises': 440,\n",
       " 'Put': 441,\n",
       " 'Quit': 442,\n",
       " 'Read': 443,\n",
       " 'Reading': 444,\n",
       " 'Really': 445,\n",
       " 'Red': 446,\n",
       " 'Release': 447,\n",
       " 'Remember': 448,\n",
       " 'Restart': 449,\n",
       " 'Rice': 450,\n",
       " 'Roll': 451,\n",
       " 'Rome': 452,\n",
       " 'Room': 453,\n",
       " 'Run': 454,\n",
       " 'Russian': 455,\n",
       " 'Sales': 456,\n",
       " 'Sapporo': 457,\n",
       " 'Saturday': 458,\n",
       " 'Saturn': 459,\n",
       " 'Save': 460,\n",
       " 'Say': 461,\n",
       " 'Scary': 462,\n",
       " 'School': 463,\n",
       " 'Science': 464,\n",
       " 'Seattle': 465,\n",
       " 'Second': 466,\n",
       " 'See': 467,\n",
       " 'Seine': 468,\n",
       " 'Selling': 469,\n",
       " 'Send': 470,\n",
       " 'Sensing': 471,\n",
       " 'September': 472,\n",
       " 'Shake': 473,\n",
       " 'Shall': 474,\n",
       " 'Shame': 475,\n",
       " 'Shanghai': 476,\n",
       " 'She': 477,\n",
       " 'Sheep': 478,\n",
       " 'Shikoku': 479,\n",
       " 'Shizuoka': 480,\n",
       " 'Should': 481,\n",
       " 'Show': 482,\n",
       " 'Shut': 483,\n",
       " 'Sing': 484,\n",
       " 'Singapore': 485,\n",
       " 'Singing': 486,\n",
       " 'Sit': 487,\n",
       " 'Six': 488,\n",
       " 'Skating': 489,\n",
       " 'Skip': 490,\n",
       " 'Skipper': 491,\n",
       " 'Slow': 492,\n",
       " 'Smith': 493,\n",
       " 'Smoke': 494,\n",
       " 'Smoking': 495,\n",
       " 'So': 496,\n",
       " 'Soccer': 497,\n",
       " 'Some': 498,\n",
       " 'Somebody': 499,\n",
       " 'Someone': 500,\n",
       " 'Something': 501,\n",
       " 'Sometimes': 502,\n",
       " 'Sorry': 503,\n",
       " 'South': 504,\n",
       " 'Spanish': 505,\n",
       " 'Speak': 506,\n",
       " 'Speaking': 507,\n",
       " 'Spell': 508,\n",
       " 'Sport': 509,\n",
       " 'Spot': 510,\n",
       " 'Spring': 511,\n",
       " 'Stadium': 512,\n",
       " 'Stand': 513,\n",
       " 'States': 514,\n",
       " 'Stay': 515,\n",
       " 'Step': 516,\n",
       " 'Stick': 517,\n",
       " 'Stir': 518,\n",
       " 'Stop': 519,\n",
       " 'Straighten': 520,\n",
       " 'Strike': 521,\n",
       " 'Study': 522,\n",
       " 'Stuff': 523,\n",
       " 'Such': 524,\n",
       " 'Suddenly': 525,\n",
       " 'Summer': 526,\n",
       " 'Summers': 527,\n",
       " 'Sunday': 528,\n",
       " 'Sundays': 529,\n",
       " 'Swedish': 530,\n",
       " 'Sweet': 531,\n",
       " 'Swimming': 532,\n",
       " 'Sydney': 533,\n",
       " 'TV': 534,\n",
       " 'Taiwan': 535,\n",
       " 'Takasu': 536,\n",
       " 'Take': 537,\n",
       " 'Tate': 538,\n",
       " 'Teachers': 539,\n",
       " 'Tears': 540,\n",
       " 'Tell': 541,\n",
       " 'Ten': 542,\n",
       " 'Tennis': 543,\n",
       " 'Thai': 544,\n",
       " 'Thames': 545,\n",
       " 'Thank': 546,\n",
       " 'Thanks': 547,\n",
       " 'That': 548,\n",
       " 'The': 549,\n",
       " 'Their': 550,\n",
       " 'There': 551,\n",
       " 'These': 552,\n",
       " 'They': 553,\n",
       " 'This': 554,\n",
       " 'Those': 555,\n",
       " 'Thou': 556,\n",
       " 'Three': 557,\n",
       " 'Throw': 558,\n",
       " 'Thursday': 559,\n",
       " 'Tickets': 560,\n",
       " 'Time': 561,\n",
       " 'Today': 562,\n",
       " 'Tokyo': 563,\n",
       " 'Tom': 564,\n",
       " 'Tomorrow': 565,\n",
       " 'Tottori': 566,\n",
       " 'Tower': 567,\n",
       " 'Training': 568,\n",
       " 'Treat': 569,\n",
       " 'Trees': 570,\n",
       " 'Trim': 571,\n",
       " 'Trouble': 572,\n",
       " 'True': 573,\n",
       " 'Trust': 574,\n",
       " 'Try': 575,\n",
       " 'Tuesday': 576,\n",
       " 'Tulips': 577,\n",
       " 'Turkey': 578,\n",
       " 'Turkish': 579,\n",
       " 'Turn': 580,\n",
       " 'Turtles': 581,\n",
       " 'Twitter': 582,\n",
       " 'Two': 583,\n",
       " 'UFO': 584,\n",
       " 'Umbrellas': 585,\n",
       " 'Unbelievable': 586,\n",
       " 'Uncle': 587,\n",
       " 'Underage': 588,\n",
       " 'Unfortunately': 589,\n",
       " 'United': 590,\n",
       " 'Use': 591,\n",
       " 'Valuable': 592,\n",
       " 'Vietnam': 593,\n",
       " 'Volleyball': 594,\n",
       " 'Vote': 595,\n",
       " 'Wait': 596,\n",
       " 'Waiter': 597,\n",
       " 'Wake': 598,\n",
       " 'War': 599,\n",
       " 'Was': 600,\n",
       " 'Wash': 601,\n",
       " 'Waste': 602,\n",
       " 'Watch': 603,\n",
       " 'Water': 604,\n",
       " 'We': 605,\n",
       " 'Wearing': 606,\n",
       " 'Wednesday': 607,\n",
       " 'Welcome': 608,\n",
       " 'Well': 609,\n",
       " 'Were': 610,\n",
       " 'What': 611,\n",
       " 'When': 612,\n",
       " 'Where': 613,\n",
       " 'Which': 614,\n",
       " 'Who': 615,\n",
       " 'Whose': 616,\n",
       " 'Why': 617,\n",
       " 'Wi-Fi': 618,\n",
       " 'Will': 619,\n",
       " 'Winds': 620,\n",
       " 'Wine': 621,\n",
       " 'Winners': 622,\n",
       " 'Winter': 623,\n",
       " 'Wipe': 624,\n",
       " 'Without': 625,\n",
       " 'Wo': 626,\n",
       " 'Wolves': 627,\n",
       " 'Wonderful': 628,\n",
       " 'Wood': 629,\n",
       " 'Words': 630,\n",
       " 'Work': 631,\n",
       " 'World': 632,\n",
       " 'Would': 633,\n",
       " 'Wow': 634,\n",
       " 'Write': 635,\n",
       " 'Year': 636,\n",
       " 'Years': 637,\n",
       " 'Yes': 638,\n",
       " 'Yesterday': 639,\n",
       " 'Yokohama': 640,\n",
       " 'York': 641,\n",
       " 'You': 642,\n",
       " 'Your': 643,\n",
       " 'Yours': 644,\n",
       " 'Zero': 645,\n",
       " '``': 646,\n",
       " 'a': 647,\n",
       " 'a.m': 648,\n",
       " 'abandoned': 649,\n",
       " 'abated': 650,\n",
       " 'abducted': 651,\n",
       " 'abilities': 652,\n",
       " 'able': 653,\n",
       " 'aboard': 654,\n",
       " 'about': 655,\n",
       " 'above': 656,\n",
       " 'abroad': 657,\n",
       " 'absconded': 658,\n",
       " 'absent': 659,\n",
       " 'absolute': 660,\n",
       " 'absolutely': 661,\n",
       " 'absorbed': 662,\n",
       " 'absorbs': 663,\n",
       " 'abstained': 664,\n",
       " 'abstract': 665,\n",
       " 'accelerated': 666,\n",
       " 'accept': 667,\n",
       " 'accepted': 668,\n",
       " 'accident': 669,\n",
       " 'accompanied': 670,\n",
       " 'accomplish': 671,\n",
       " 'accomplished': 672,\n",
       " 'account': 673,\n",
       " 'accurate': 674,\n",
       " 'accused': 675,\n",
       " 'accustomed': 676,\n",
       " 'ache': 677,\n",
       " 'aches': 678,\n",
       " 'acid': 679,\n",
       " 'acknowledged': 680,\n",
       " 'acorns': 681,\n",
       " 'acquainted': 682,\n",
       " 'acres': 683,\n",
       " 'across': 684,\n",
       " 'act': 685,\n",
       " 'acted': 686,\n",
       " 'active': 687,\n",
       " 'activity': 688,\n",
       " 'actress': 689,\n",
       " 'acts': 690,\n",
       " 'actually': 691,\n",
       " 'add': 692,\n",
       " 'addict': 693,\n",
       " 'address': 694,\n",
       " 'admire': 695,\n",
       " 'admired': 696,\n",
       " 'admit': 697,\n",
       " 'admitted': 698,\n",
       " 'adopted': 699,\n",
       " 'adorable': 700,\n",
       " 'adores': 701,\n",
       " 'adult': 702,\n",
       " 'advance': 703,\n",
       " 'adventure': 704,\n",
       " 'advertised': 705,\n",
       " 'advice': 706,\n",
       " 'advised': 707,\n",
       " 'affair': 708,\n",
       " 'affairs': 709,\n",
       " 'affection': 710,\n",
       " 'affects': 711,\n",
       " 'afford': 712,\n",
       " 'afraid': 713,\n",
       " 'after': 714,\n",
       " 'afternoon': 715,\n",
       " 'afterwards': 716,\n",
       " 'again': 717,\n",
       " 'against': 718,\n",
       " 'age': 719,\n",
       " 'ages': 720,\n",
       " 'aggressive': 721,\n",
       " 'ago': 722,\n",
       " 'agree': 723,\n",
       " 'agreeable': 724,\n",
       " 'agreed': 725,\n",
       " 'agrees': 726,\n",
       " 'ahead': 727,\n",
       " 'aimed': 728,\n",
       " 'air': 729,\n",
       " 'air-conditioned': 730,\n",
       " 'airplane': 731,\n",
       " 'airport': 732,\n",
       " 'airtight': 733,\n",
       " 'alarm': 734,\n",
       " 'album': 735,\n",
       " 'alcohol': 736,\n",
       " 'alcoholic': 737,\n",
       " 'alert': 738,\n",
       " 'aliens': 739,\n",
       " 'alike': 740,\n",
       " 'alive': 741,\n",
       " 'all': 742,\n",
       " 'allergic': 743,\n",
       " 'allow': 744,\n",
       " 'allowed': 745,\n",
       " 'almost': 746,\n",
       " 'alone': 747,\n",
       " 'along': 748,\n",
       " 'already': 749,\n",
       " 'also': 750,\n",
       " 'always': 751,\n",
       " 'am': 752,\n",
       " 'amazed': 753,\n",
       " 'amazes': 754,\n",
       " 'amazing': 755,\n",
       " 'amazingly': 756,\n",
       " 'ambiguous': 757,\n",
       " 'ambulance': 758,\n",
       " 'amount': 759,\n",
       " 'an': 760,\n",
       " 'analyzed': 761,\n",
       " 'and': 762,\n",
       " 'angel': 763,\n",
       " 'anger': 764,\n",
       " 'angry': 765,\n",
       " 'animal': 766,\n",
       " 'animals': 767,\n",
       " 'annoyed': 768,\n",
       " 'annoying': 769,\n",
       " 'another': 770,\n",
       " 'answer': 771,\n",
       " 'answered': 772,\n",
       " 'anticipate': 773,\n",
       " 'antiques': 774,\n",
       " 'anxious': 775,\n",
       " 'any': 776,\n",
       " 'anybody': 777,\n",
       " 'anymore': 778,\n",
       " 'anyone': 779,\n",
       " 'anything': 780,\n",
       " 'anytime': 781,\n",
       " 'anyway': 782,\n",
       " 'anywhere': 783,\n",
       " 'apart': 784,\n",
       " 'apartment': 785,\n",
       " 'apologetic': 786,\n",
       " 'apologize': 787,\n",
       " 'apologized': 788,\n",
       " 'apology': 789,\n",
       " 'app': 790,\n",
       " 'appeal': 791,\n",
       " 'appeared': 792,\n",
       " 'appears': 793,\n",
       " 'appetite': 794,\n",
       " 'apple': 795,\n",
       " 'apples': 796,\n",
       " 'applied': 797,\n",
       " 'appointed': 798,\n",
       " 'appreciate': 799,\n",
       " 'appreciates': 800,\n",
       " 'approaching': 801,\n",
       " 'approval': 802,\n",
       " 'approve': 803,\n",
       " 'approved': 804,\n",
       " 'apt': 805,\n",
       " 'architect': 806,\n",
       " 'are': 807,\n",
       " 'area': 808,\n",
       " 'argue': 809,\n",
       " 'arguing': 810,\n",
       " 'arisen': 811,\n",
       " 'arm': 812,\n",
       " 'armed': 813,\n",
       " 'arms': 814,\n",
       " 'army': 815,\n",
       " 'around': 816,\n",
       " 'arrested': 817,\n",
       " 'arrests': 818,\n",
       " 'arrive': 819,\n",
       " 'arrived': 820,\n",
       " 'arrogance': 821,\n",
       " 'arrow': 822,\n",
       " 'art': 823,\n",
       " 'article': 824,\n",
       " 'artificial': 825,\n",
       " 'artist': 826,\n",
       " 'artists': 827,\n",
       " 'as': 828,\n",
       " 'ashamed': 829,\n",
       " 'aside': 830,\n",
       " 'ask': 831,\n",
       " 'asked': 832,\n",
       " 'asking': 833,\n",
       " 'asleep': 834,\n",
       " 'assigned': 835,\n",
       " 'assist': 836,\n",
       " 'assistant': 837,\n",
       " 'assume': 838,\n",
       " 'astonished': 839,\n",
       " 'at': 840,\n",
       " 'ate': 841,\n",
       " 'athletic': 842,\n",
       " 'atrocious': 843,\n",
       " 'attacked': 844,\n",
       " 'attempted': 845,\n",
       " 'attend': 846,\n",
       " 'attended': 847,\n",
       " 'attention': 848,\n",
       " 'attic': 849,\n",
       " 'attractive': 850,\n",
       " 'audacity': 851,\n",
       " 'audience': 852,\n",
       " 'aunt': 853,\n",
       " 'author': 854,\n",
       " 'automatically': 855,\n",
       " 'available': 856,\n",
       " 'average': 857,\n",
       " 'avoidable': 858,\n",
       " 'awake': 859,\n",
       " 'aware': 860,\n",
       " 'away': 861,\n",
       " 'awesome': 862,\n",
       " 'awful': 863,\n",
       " 'awfully': 864,\n",
       " 'awkward': 865,\n",
       " 'awoke': 866,\n",
       " 'baby': 867,\n",
       " 'back': 868,\n",
       " 'backed': 869,\n",
       " 'backward': 870,\n",
       " 'bad': 871,\n",
       " 'bad-looking': 872,\n",
       " 'badly': 873,\n",
       " 'bag': 874,\n",
       " 'baggage': 875,\n",
       " 'bags': 876,\n",
       " 'baked': 877,\n",
       " 'ball': 878,\n",
       " 'band': 879,\n",
       " 'bank': 880,\n",
       " 'bankrupt': 881,\n",
       " 'barber': 882,\n",
       " 'barely': 883,\n",
       " 'bargain': 884,\n",
       " 'bark': 885,\n",
       " 'barked': 886,\n",
       " 'baseball': 887,\n",
       " 'basic': 888,\n",
       " 'basketball': 889,\n",
       " 'bath': 890,\n",
       " 'bathe': 891,\n",
       " 'bathroom': 892,\n",
       " 'battle': 893,\n",
       " 'be': 894,\n",
       " 'beach': 895,\n",
       " 'beans': 896,\n",
       " 'bear': 897,\n",
       " 'beard': 898,\n",
       " 'bears': 899,\n",
       " 'beat': 900,\n",
       " 'beautiful': 901,\n",
       " 'beauty': 902,\n",
       " 'became': 903,\n",
       " 'because': 904,\n",
       " 'beckoned': 905,\n",
       " 'become': 906,\n",
       " 'bed': 907,\n",
       " 'bedroom': 908,\n",
       " 'bedrooms': 909,\n",
       " 'bee': 910,\n",
       " 'beef': 911,\n",
       " 'been': 912,\n",
       " 'beer': 913,\n",
       " 'beers': 914,\n",
       " 'before': 915,\n",
       " 'began': 916,\n",
       " 'beggar': 917,\n",
       " 'begin': 918,\n",
       " 'beginning': 919,\n",
       " 'begins': 920,\n",
       " 'begun': 921,\n",
       " 'behaved': 922,\n",
       " 'behavior': 923,\n",
       " 'behind': 924,\n",
       " 'being': 925,\n",
       " 'believe': 926,\n",
       " 'believes': 927,\n",
       " 'bell': 928,\n",
       " 'belly': 929,\n",
       " 'belong': 930,\n",
       " 'belonged': 931,\n",
       " 'belongs': 932,\n",
       " 'below': 933,\n",
       " 'belt': 934,\n",
       " 'belts': 935,\n",
       " 'bench': 936,\n",
       " 'bend': 937,\n",
       " 'beside': 938,\n",
       " 'best': 939,\n",
       " 'bet': 940,\n",
       " 'betray': 941,\n",
       " 'better': 942,\n",
       " 'between': 943,\n",
       " 'bicycle': 944,\n",
       " 'bid': 945,\n",
       " 'big': 946,\n",
       " 'bigger': 947,\n",
       " 'bike': 948,\n",
       " 'bill': 949,\n",
       " 'biology': 950,\n",
       " 'bird': 951,\n",
       " 'birds': 952,\n",
       " 'birth': 953,\n",
       " 'birthday': 954,\n",
       " 'bit': 955,\n",
       " 'bite': 956,\n",
       " 'bites': 957,\n",
       " 'bitten': 958,\n",
       " 'bitter': 959,\n",
       " 'bitterly': 960,\n",
       " 'bizarre': 961,\n",
       " 'black': 962,\n",
       " 'blame': 963,\n",
       " 'blamed': 964,\n",
       " 'blanket': 965,\n",
       " 'blast': 966,\n",
       " 'bleed': 967,\n",
       " 'bleeding': 968,\n",
       " 'bless': 969,\n",
       " 'blessed': 970,\n",
       " 'blessing': 971,\n",
       " 'blew': 972,\n",
       " 'blind': 973,\n",
       " 'bliss': 974,\n",
       " 'blisters': 975,\n",
       " 'blocked': 976,\n",
       " 'blog': 977,\n",
       " 'blood': 978,\n",
       " 'bloom': 979,\n",
       " 'blue': 980,\n",
       " 'blushed': 981,\n",
       " 'blushing': 982,\n",
       " 'board': 983,\n",
       " 'boarded': 984,\n",
       " 'boat': 985,\n",
       " 'boil': 986,\n",
       " 'boiled': 987,\n",
       " 'book': 988,\n",
       " 'booked': 989,\n",
       " 'bookkeeping': 990,\n",
       " 'books': 991,\n",
       " 'bookshelf': 992,\n",
       " 'boots': 993,\n",
       " 'border': 994,\n",
       " 'borders': 995,\n",
       " 'bored': 996,\n",
       " 'boring': 997,\n",
       " 'born': 998,\n",
       " 'borrow': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 437s 55ms/step - loss: 1.3211 - val_loss: 1.7009\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 482s 60ms/step - loss: 1.2391 - val_loss: 1.6733\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 473s 59ms/step - loss: 1.2033 - val_loss: 1.6492\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 450s 56ms/step - loss: 1.1691 - val_loss: 1.6396\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 432s 54ms/step - loss: 1.1382 - val_loss: 1.6467\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 431s 54ms/step - loss: 1.1051 - val_loss: 1.5956\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 479s 60ms/step - loss: 1.0698 - val_loss: 1.5868\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 467s 58ms/step - loss: 1.0386 - val_loss: 1.5709\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 469s 59ms/step - loss: 1.0072 - val_loss: 1.5593\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 474s 59ms/step - loss: 0.9781 - val_loss: 1.5491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:2368: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    #target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: He ran.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: I quit.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Try it.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: We try.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Why me?\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Be kind.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Get Tom.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Get out!\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Go away.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Goodbye!\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Goodbye!\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Hang on!\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: He came.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: He runs.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Help me.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Hold on.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Hug Tom.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: I agree.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: I'm ill.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: I'm old.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: It's OK.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: It's me.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Join us.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Keep it.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Kiss me.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Perfect!\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: See you.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Shut up!\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Skip it.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Take it.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Wake up!\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Wash up.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: We know.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Welcome.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Who won?\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Why not?\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: You run.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Back off.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Be still.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Cuff him.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Drive on.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Get away!\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Get away!\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Get down!\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Get lost!\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Get real.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Grab Tom.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Grab him.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Have fun.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: He tries.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Humor me.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Hurry up.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Hurry up.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: I forgot.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: I resign.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: I'll pay.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: I'm busy.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: I'm cold.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: I'm fine.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: I'm full.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: I'm sick.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: I'm sick.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Leave me.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Let's go!\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Let's go!\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Let's go!\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Look out!\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: She runs.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Stand up.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: They won.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Tom died.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Tom quit.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Tom swam.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Trust me.\n",
      "Decoded sentence: \n",
      "-\n",
      "Input sentence: Try hard.\n",
      "Decoded sentence: \n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
